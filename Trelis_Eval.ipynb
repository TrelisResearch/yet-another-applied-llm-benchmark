{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sddGmEyxMegT"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlini/yet-another-applied-llm-benchmark/blob/master/run_a_simple_testcase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trelis Eval\n",
        "\n",
        "This notebook is built upon the work of [Nicolas Carlini](https://nicholas.carlini.com/).\n",
        "\n",
        "The Trelis forked repo is [here](https://github.com/TrelisResearch/yet-another-applied-llm-benchmark) and has been modified to support custom endpoints.\n",
        "\n",
        "Carlini's original repo is [here](https://github.com/carlini/yet-another-applied-llm-benchmark)"
      ],
      "metadata": {
        "id": "vy8HTHjqju78"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35QuPBmFNBL2"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A_XJDB0NAEb",
        "outputId": "dadeebc4-a30d-4fad-f317-699c0168b6e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yet-another-applied-llm-benchmark'...\n",
            "remote: Enumerating objects: 826, done.\u001b[K\n",
            "remote: Counting objects: 100% (364/364), done.\u001b[K\n",
            "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
            "remote: Total 826 (delta 280), reused 307 (delta 232), pack-reused 462\u001b[K\n",
            "Receiving objects: 100% (826/826), 739.64 KiB | 14.50 MiB/s, done.\n",
            "Resolving deltas: 100% (599/599), done.\n",
            "/content/yet-another-applied-llm-benchmark\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.6/147.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.2/460.2 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m846.4/846.4 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/carlini/yet-another-applied-llm-benchmark.git\n",
        "!git clone https://github.com/TrelisResearch/yet-another-applied-llm-benchmark.git\n",
        "\n",
        "%cd yet-another-applied-llm-benchmark\n",
        "!pip install -r requirements.txt -q -U\n",
        "!pip install -r requirements-extra.txt -q -U\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CJ44Re4nSoTd"
      },
      "outputs": [],
      "source": [
        "# !sudo apt-get install podman"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s9X1rz69SZfw"
      },
      "outputs": [],
      "source": [
        "# !which podman"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since colab is self contained, we can run directly in the environment:\n",
        "\n",
        "In `docker_controller.py` (if not using `podman` (or) `docker`):\n",
        "1. Set `I_HAVE_BLIND_FAITH_IN_LLMS_AND_AM_OKAY_WITH_THEM_BRICKING_MY_MACHINE_OR_MAKING_THEM_HALT_AND_CATCH_FIRE` to `True`\n",
        "\n",
        "In `evaluator.py`:\n",
        "1. Set PYTHON_ENV = \"python\"\n",
        "\n",
        "In `llm.py`:\n",
        "1. Set llm = LLM(\"gpt-3.5-turbo\").\n",
        "1. By default, gpt4 will be used for evaluation.\n",
        "\n",
        "DO NOT DO THIS IF YOU ARE RUNNING ON A LOCAL MACHINE."
      ],
      "metadata": {
        "id": "86vkbR__lmJ-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTE6bJpJUo4R"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K31AsKNWNXuB"
      },
      "source": [
        "### Add API keys\n",
        "\n",
        "You have to add the api keys of the respective models to run them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A8OwQ_PNOcK",
        "outputId": "55003016-6b46-4563-fd03-58a2c9a56449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.json\n",
        "{\n",
        "    \"container\": \"podman\",\n",
        "    \"hparams\": {\n",
        "        \"temperature\": 0.1\n",
        "    },\n",
        "    \"llms\": {\n",
        "        \"Mistral-7B-Instruct-v0.1\": {\n",
        "            \"api_key\": \"EMPTY\",\n",
        "            \"endpoint\": \"https://noxh1e30xaiuv1-8080.proxy.runpod.net/v1\",\n",
        "            \"slug\": \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "        },\n",
        "        \"Mistral-7B-Instruct-v0.1-GGUF\": {\n",
        "            \"api_key\": \"EMPTY\",\n",
        "            \"endpoint\": \"https://<>-8080.proxy.runpod.net/v1\",\n",
        "            \"slug\": \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\n",
        "        },\n",
        "        \"Mistral-7B-Instruct-v0.1-AWQ\": {\n",
        "            \"api_key\": \"EMPTY\",\n",
        "            \"endpoint\": \"https://<>-8080.proxy.runpod.net/v1\",\n",
        "            \"slug\": \"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\"\n",
        "        },\n",
        "        \"openchat_3.5\": {\n",
        "            \"api_key\": \"EMPTY\",\n",
        "            \"endpoint\": \"https://czpagcno0qmqku-8000.proxy.runpod.net/v1\",\n",
        "            \"slug\": \"openchat/openchat_3.5\"\n",
        "        },\n",
        "        \"gemma-7b\": {\n",
        "            \"api_key\": \"EMPTY\",\n",
        "            \"endpoint\": \"https://<>-8000.proxy.runpod.net/v1\",\n",
        "            \"slug\": \"google/gemma-7b\"\n",
        "        },\n",
        "        \"Qwen1.5-7B-Chat\": {\n",
        "            \"api_key\": \"EMPTY\",\n",
        "            \"endpoint\": \"https://ulvtf52fdafeuv-8000.proxy.runpod.net/v1\",\n",
        "            \"slug\": \"Qwen/Qwen1.5-7B-Chat\"\n",
        "        },\n",
        "        \"Qwen1.5-72B-Chat\": {\n",
        "            \"api_key\": \"EMPTY\",\n",
        "            \"endpoint\": \"https://<>-8000.proxy.runpod.net/v1\",\n",
        "            \"slug\": \"Qwen/Qwen1.5-72B-Chat\"\n",
        "        },\n",
        "        \"mistral\": {\n",
        "            \"api_key\": \"abc\"\n",
        "        },\n",
        "        \"openai\": {\n",
        "            \"api_key\": \"sk-def\"\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-9U4FNoUe6I"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr2FPWYUjoGY"
      },
      "source": [
        "Now, let's run a simple test case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUC4OCZROW81",
        "outputId": "5a5b5057-d7b6-4399-e7ea-d71cbe6667b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-27 17:41:11.745339: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-27 17:41:11.745440: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-27 17:41:11.747767: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-27 17:41:11.760753: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-02-27 17:41:13.513852: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Using a model name of Qwen1.5-7B-Chat\n",
            "model is: Qwen/Qwen1.5-7B-Chat\n",
            "\n",
            "endpoint is https://ulvtf52fdafeuv-8000.proxy.runpod.net/v1\n",
            "Qwen1.5-7B-Chat CACHE MISS ['Write a python program that prints the string \"hello world\" and tell me how it works in a sentence']\n",
            "# Initial Query\n",
            "> Write a python program that prints the string \"hello world\" and tell me how it works in a sentence\n",
            "\n",
            "# LLM Generation\n",
            "\n",
            "## Query\n",
            "> Write a python program that prints the string \"hello world\" and tell me how it works in a sentence\n",
            "\n",
            "## Output\n",
            "> Sure! Here's a simple Python program that prints the string \"hello world\" to the console:\n",
            "> \n",
            "> ```python\n",
            "> print(\"hello world\")\n",
            "> ```\n",
            "> \n",
            "> This program works by using the `print()` function, which is a built-in function in Python. It takes a string as an argument and outputs it to the screen. In this case, the string \"hello world\" is passed to `print()`, and it gets printed verbatim.\n",
            "> \n",
            "> When you run this program, you'll see the output `hello world` displayed in the console.\n",
            "\n",
            "# Extract Code\n",
            "I extracted the following code from that output:\n",
            "> ```\n",
            "> print(\"hello world\")\n",
            "> ```\n",
            "\n",
            "# Run Code Interpreter\n",
            "Running the following program:\n",
            "> ```\n",
            "> print(\"hello world\")\n",
            "> ```\n",
            "And got the output:\n",
            "```\n",
            "hello world\n",
            "```\n",
            "\n",
            "# Substring Evaluation\n",
            "Testing if the previous output contains the string `hello world`: True\n",
            "\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "!PYTHONPATH='.' python tests/print_hello.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a Full Evaluation\n",
        "You need to ensure your model is updated in llm.py, in config.json and also that you have set api keys at least for openai and any model you wish to evaluate.\n",
        "\n",
        "Further, in main.py you need to specify the models you wish to evaluate when --all-models is set as a flag."
      ],
      "metadata": {
        "id": "wiZEi6RPnGsL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH1T2iv2SuRh"
      },
      "outputs": [],
      "source": [
        "!python main.py --all-models --run-tests --generate-report"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}